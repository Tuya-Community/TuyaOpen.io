"use strict";(self.webpackChunktuyaopen_io_website=self.webpackChunktuyaopen_io_website||[]).push([["3776"],{60196:function(e,n,i){i.r(n),i.d(n,{frontMatter:()=>a,toc:()=>d,default:()=>c,metadata:()=>t,assets:()=>s,contentTitle:()=>r});var t=JSON.parse('{"id":"applications/tuya.ai/ai-components/ai-audio-asr-impl","title":"AI Voice Interaction Implementation","description":"Terminology","source":"@site/docs/applications/tuya.ai/ai-components/ai-audio-asr-impl.md","sourceDirName":"applications/tuya.ai/ai-components","slug":"/applications/tuya.ai/ai-components/ai-audio-asr-impl","permalink":"/docs/applications/tuya.ai/ai-components/ai-audio-asr-impl","draft":false,"unlisted":false,"editUrl":"https://github.com/Tuya-Community/TuyaOpen.io/edit/master/docs/applications/tuya.ai/ai-components/ai-audio-asr-impl.md","tags":[],"version":"current","frontMatter":{"title":"AI Voice Interaction Implementation"},"sidebar":"docs","previous":{"title":"Duo-Eyes Mood Robot","permalink":"/docs/applications/tuya.ai/demo-duo-eyes-mood"},"next":{"title":"Generic Demos","permalink":"/docs/examples/demo-generic-examples"}}'),o=i(74848),l=i(84429);let a={title:"AI Voice Interaction Implementation"},r=void 0,s={},d=[{value:"Terminology",id:"terminology",level:2},{value:"Function Overview",id:"function-overview",level:2},{value:"Functional Modules",id:"functional-modules",level:2},{value:"Workflow",id:"workflow",level:2},{value:"Manually Triggered Single-Turn Dialogue",id:"manually-triggered-single-turn-dialogue",level:3},{value:"VAD Triggered Free Dialogue",id:"vad-triggered-free-dialogue",level:3},{value:"ASR Wakeup Single-Turn Dialogue",id:"asr-wakeup-single-turn-dialogue",level:3},{value:"ASR Wakeup Free Dialogue",id:"asr-wakeup-free-dialogue",level:3},{value:"Development Process",id:"development-process",level:2},{value:"Structure Description",id:"structure-description",level:3},{value:"Working Modes",id:"working-modes",level:4},{value:"Event Types",id:"event-types",level:4},{value:"Component States",id:"component-states",level:4},{value:"API Description",id:"api-description",level:3},{value:"Module Initialization",id:"module-initialization",level:4},{value:"Open Audio Module",id:"open-audio-module",level:4},{value:"Set Volume",id:"set-volume",level:4},{value:"Get Volume",id:"get-volume",level:4},{value:"Manually Start Voice Input",id:"manually-start-voice-input",level:4},{value:"Manually Stop Voice Input",id:"manually-stop-voice-input",level:4},{value:"Wakeup Module",id:"wakeup-module",level:4},{value:"Get Module State",id:"get-module-state",level:4},{value:"Play Built-in Prompt Sound",id:"play-built-in-prompt-sound",level:4},{value:"Development Steps",id:"development-steps",level:3}];function u(e){let n={code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"terminology",children:"Terminology"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Term"}),(0,o.jsx)(n.th,{children:"Explanation"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"VAD"}),(0,o.jsx)(n.td,{children:"Voice Activity Detection, a technology used to determine whether there is speech in an audio signal."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"ASR"}),(0,o.jsx)(n.td,{children:"Automatic Speech Recognition, a technology that converts speech content into computer-recognizable text or commands."})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"function-overview",children:"Function Overview"}),"\n",(0,o.jsx)(n.p,{children:"ai_audio is mainly used to handle operations related to AI and audio, including audio input, output, configuration management, and creating AI sessions. The following is a detailed description of the component's functions:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Collect audio data"}),"\n",(0,o.jsx)(n.li,{children:"Play audio data"}),"\n",(0,o.jsx)(n.li,{children:"Create a cloud AI session, send the collected valid data to the cloud for ASR recognition, and the cloud will reply based on the ASR result"}),"\n",(0,o.jsxs)(n.li,{children:["Preprocess the collected audio data, send it to the cloud only after valid content is detected, reducing the cloud processing load.","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"VAD detection"}),"\n",(0,o.jsx)(n.li,{children:"ASR wake word detection"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Provides four ",(0,o.jsx)(n.strong,{children:"working modes"})," according to different ",(0,o.jsx)(n.strong,{children:"dialogue modes"})," and ",(0,o.jsx)(n.strong,{children:"trigger methods"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Dialogue mode","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Single-turn dialogue: Each trigger only performs one round of Q&A."}),"\n",(0,o.jsx)(n.li,{children:"Free dialogue: Each trigger allows N rounds of continuous dialogue."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Dialogue trigger method","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Manual control, such as pressing and holding a button."}),"\n",(0,o.jsx)(n.li,{children:"VAD detection: Start dialogue when voice is detected."}),"\n",(0,o.jsx)(n.li,{children:"Local ASR wake word detection: Start dialogue when the wake word is detected."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Working modes","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Manually triggered single-turn dialogue"}),"\n",(0,o.jsx)(n.li,{children:"VAD triggered free dialogue"}),"\n",(0,o.jsx)(n.li,{children:"ASR wakeup single-turn dialogue"}),"\n",(0,o.jsx)(n.li,{children:"ASR wakeup free dialogue"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"functional-modules",children:"Functional Modules"}),"\n",(0,o.jsx)(n.p,{children:"This component mainly consists of five functional modules:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Audio Input Module (input)","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Collect audio data"}),"\n",(0,o.jsx)(n.li,{children:"Audio data preprocessing"}),"\n",(0,o.jsx)(n.li,{children:"Module state change notification"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["AI Agent Module (ai agent)","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Create cloud session"}),"\n",(0,o.jsxs)(n.li,{children:["Data reporting, ",(0,o.jsx)(n.strong,{children:"default format: PCM (OPUS optional)"})]}),"\n",(0,o.jsxs)(n.li,{children:["Receive cloud data, ",(0,o.jsx)(n.strong,{children:"default format: MP3, 16bit, 16KHz, mono"})]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Cloud ASR Processing Module (cloud asr)","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Start reporting"}),"\n",(0,o.jsx)(n.li,{children:"End reporting"}),"\n",(0,o.jsx)(n.li,{children:"Wait for cloud ASR"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Audio Playback Module (player)","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Play audio data returned from the cloud"}),"\n",(0,o.jsx)(n.li,{children:"Play built-in prompt sounds"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Management Module (main)","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Component entry"}),"\n",(0,o.jsx)(n.li,{children:"Manage the above four modules"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"workflow",children:"Workflow"}),"\n",(0,o.jsx)(n.h3,{id:"manually-triggered-single-turn-dialogue",children:"Manually Triggered Single-Turn Dialogue"}),"\n",(0,o.jsx)(n.p,{children:"Under external conditions, the user can initiate a dialogue. Each trigger only performs one round of Q&A. For example, when the button is pressed, the user can input speech, and releasing the button indicates the end of speech input, then wait for the AI to reply."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:'usr: "Who are you" (under some external condition, e.g., button pressed)\nai : "I am xxx"\nusr: "What\'s the weather today" (under some external condition, e.g., button pressed)\nai : "xxxx"\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:i(58756).A+"",width:"1467",height:"1073"})}),"\n",(0,o.jsx)(n.h3,{id:"vad-triggered-free-dialogue",children:"VAD Triggered Free Dialogue"}),"\n",(0,o.jsx)(n.p,{children:"The device sends the collected audio data to the VAD module for human voice detection. If human voice is detected, the session is considered started. That is, the user can speak at any time, and the module will send the user's voice data to the cloud to initiate a session."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:'usr: "Who are you"\nai : "I am xxx"\nusr: "What\'s the weather today"\nai : "xxxx"\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:i(22918).A+"",width:"1467",height:"971"})}),"\n",(0,o.jsx)(n.h3,{id:"asr-wakeup-single-turn-dialogue",children:"ASR Wakeup Single-Turn Dialogue"}),"\n",(0,o.jsx)(n.p,{children:"The user needs to say the wake word before each dialogue to wake up the device. After each wakeup, the user can only initiate one dialogue. After the dialogue ends, the user needs to say the wake word again to start a new dialogue, similar to a smart speaker mode."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:'usr: "Hello, xxxx" (wake word)\nai : "I\'m here" (prompt sound)\nusr: "Who are you"\nai : "I am xxx"\nusr: "Hello, xxxx" (wake word)\nai : "I\'m here" (prompt sound)\nusr: "What\'s the weather today"\nai : "xxxx"\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:i(79114).A+"",width:"1533",height:"1302"})}),"\n",(0,o.jsx)(n.h3,{id:"asr-wakeup-free-dialogue",children:"ASR Wakeup Free Dialogue"}),"\n",(0,o.jsx)(n.p,{children:"After the user says the wake word to wake up the device, continuous dialogue can be carried out. After being woken up, if no sound is detected for 30 seconds, the device will re-enter the wake word detection state."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:'usr: "Hello, xxxx" (wake word)\nai : "I\'m here" (prompt sound)\nusr: "Who are you"\nai : "I am xxx"\nusr: "What\'s the weather today"\nai : "xxxx"\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:i(97317).A+"",width:"1552",height:"1334"})}),"\n",(0,o.jsx)(n.h2,{id:"development-process",children:"Development Process"}),"\n",(0,o.jsx)(n.h3,{id:"structure-description",children:"Structure Description"}),"\n",(0,o.jsx)(n.h4,{id:"working-modes",children:"Working Modes"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-C",children:"typedef uint8_t AI_AUDIO_WORK_MODE_E;\n#define AI_AUDIO_MODE_MANUAL_SINGLE_TALK     1 //Manually triggered single-turn dialogue\n#define AI_AUDIO_WORK_VAD_FREE_TALK          2 //VAD triggered free dialogue\n#define AI_AUDIO_WORK_ASR_WAKEUP_SINGLE_TALK 3 //ASR wakeup single-turn dialogue\n#define AI_AUDIO_WORK_ASR_WAKEUP_FREE_TALK   4 //ASR wakeup free dialogue\n"})}),"\n",(0,o.jsx)(n.h4,{id:"event-types",children:"Event Types"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"typedef enum {\n    AI_AUDIO_EVT_NONE,                      //None\n    AI_AUDIO_EVT_HUMAN_ASR_TEXT,            //Return user speech text\n    AI_AUDIO_EVT_AI_REPLIES_TEXT_START,     //Start transmitting AI speech text\n    AI_AUDIO_EVT_AI_REPLIES_TEXT_DATA,      //Transmit AI speech text\n    AI_AUDIO_EVT_AI_REPLIES_TEXT_END,       //End transmitting AI speech text\n    AI_AUDIO_EVT_AI_REPLIES_EMO,            //Return AI emotion\n    AI_AUDIO_EVT_ASR_WAKEUP,                //Wake word detected\n} AI_AUDIO_EVENT_E;\n\ntypedef struct {\n    char *name;\n    char *text;\n} AI_AUDIO_EMOTION_T;                       //Emotion structure\n\n//Event notification callback\ntypedef void (*AI_AUDIO_EVT_INFORM_CB)(AI_AUDIO_EVENT_E event, uint8_t *data, uint32_t len, void *arg);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"component-states",children:"Component States"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"typedef enum {\n    AI_AUDIO_STATE_STANDBY,                 //Standby state\n    AI_AUDIO_STATE_LISTEN,                  //Listening\n    AI_AUDIO_STATE_UPLOAD,                  //Uploading data to cloud\n    AI_AUDIO_STATE_AI_SPEAK,                //Playing AI speech returned from cloud\n    AI_AUDIO_STATE_MAX = 0xFF,              //Invalid state\n} AI_AUDIO_STATE_E;\n\n//State notification callback\ntypedef void (*AI_AUDIO_STATE_INFORM_CB)(AI_AUDIO_STATE_E state);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"api-description",children:"API Description"}),"\n",(0,o.jsx)(n.h4,{id:"module-initialization",children:"Module Initialization"}),"\n",(0,o.jsx)(n.p,{children:"This API is mainly used to initialize AI-related services, audio devices, and other resources."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-C",children:"typedef struct {\n    AI_AUDIO_WORK_MODE_E work_mode;\n    AI_AUDIO_EVT_INFORM_CB evt_inform_cb;\n    AI_AUDIO_STATE_INFORM_CB state_inform_cb;\n} AI_AUDIO_CONFIG_T;\n\n/**\n * @brief Initializes the audio module with the provided configuration.\n * @param cfg Pointer to the configuration structure for the audio module.\n * @return OPERATE_RET - OPRT_OK if initialization is successful, otherwise an error code.\n */\nOPERATE_RET ai_audio_init(AI_AUDIO_CONFIG_T *cfg);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"open-audio-module",children:"Open Audio Module"}),"\n",(0,o.jsx)(n.p,{children:"The audio module is off by default. The user needs to call this API to open the module."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Sets the open state of the audio module.\n * @param is_open Boolean value indicating whether to open (true) or close (false) the audio module.\n * @return OPERATE_RET - OPRT_OK if the operation is successful, otherwise an error code.\n */\nOPERATE_RET ai_audio_set_open(bool is_open);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"set-volume",children:"Set Volume"}),"\n",(0,o.jsx)(n.p,{children:"Set the microphone volume."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Sets the volume for the audio module.\n * @param volume The volume level to set.\n * @return OPERATE_RET - OPRT_OK if the volume is set successfully, otherwise an error code.\n */\nOPERATE_RET ai_audio_set_volume(uint8_t volume);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"get-volume",children:"Get Volume"}),"\n",(0,o.jsx)(n.p,{children:"Get the current microphone volume."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Retrieves the current volume setting for the audio module.\n * @param None\n * @return uint8_t - The current volume level.\n */\nuint8_t ai_audio_get_volume(void);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"manually-start-voice-input",children:"Manually Start Voice Input"}),"\n",(0,o.jsx)(n.p,{children:"After calling this API, the module enters the valid audio receiving state. By default, the subsequently collected audio data will be sent to the cloud for ASR recognition."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Manually starts a single talk session for AI audio.\n *\n * @param None\n * @return OPERATE_RET Operation result code.\n */\nOPERATE_RET ai_audio_manual_start_single_talk(void);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"manually-stop-voice-input",children:"Manually Stop Voice Input"}),"\n",(0,o.jsx)(n.p,{children:"After calling this API, the module will no longer receive valid audio. The subsequently collected audio data will not be sent to the cloud."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Manually stops a single talk session in the AI audio component.\n *\n * @return OPERATE_RET Returns the operation result. Typically, this indicates success or provides an error code.\n */\nOPERATE_RET ai_audio_manual_stop_single_talk(void);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"wakeup-module",children:"Wakeup Module"}),"\n",(0,o.jsx)(n.p,{children:"After calling this API, the module will enter the state of detecting a new round of dialogue (valid audio detection state). If the module is currently in a dialogue, the current session will be interrupted."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Sets the audio system to wakeup mode.\n *\n * This function configures the audio system to enable wakeup functionality,\n * allowing it to respond to wakeup events or keywords.\n *\n * @return OPERATE_RET Returns the operation result. Returns OPRT_OK on success, or an error code on failure.\n */\nOPERATE_RET ai_audio_set_wakeup(void);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"get-module-state",children:"Get Module State"}),"\n",(0,o.jsx)(n.p,{children:"Get the current state of the module."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-C",children:"/**\n * @brief Retrieves the current state of the AI audio system.\n *\n * @return AI_AUDIO_STATE_E The current state of the AI audio system.\n */\nAI_AUDIO_STATE_E ai_audio_get_state(void);\n"})}),"\n",(0,o.jsx)(n.h4,{id:"play-built-in-prompt-sound",children:"Play Built-in Prompt Sound"}),"\n",(0,o.jsx)(n.p,{children:"Play various built-in prompt sounds, such as network configuration status, dialogue mode, etc."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:"typedef enum {\n    AI_AUDIO_ALERT_NORMAL = 0,\n    AI_AUDIO_ALERT_POWER_ON,             /* Power on broadcast */\n    AI_AUDIO_ALERT_NOT_ACTIVE,           /* Not networked yet, please configure network first */\n    AI_AUDIO_ALERT_NETWORK_CFG,          /* Enter network configuration state, start configuring */\n    AI_AUDIO_ALERT_NETWORK_CONNECTED,    /* Network connected successfully */\n    AI_AUDIO_ALERT_NETWORK_FAIL,         /* Network connection failed, retry */\n    AI_AUDIO_ALERT_NETWORK_DISCONNECT,   /* Network disconnected */\n    AI_AUDIO_ALERT_BATTERY_LOW,          /* Low battery */\n    AI_AUDIO_ALERT_PLEASE_AGAIN,         /* Please say it again */\n    AI_AUDIO_ALERT_WAKEUP,               /* Hello, I'm here */\n    AI_AUDIO_ALERT_LONG_KEY_TALK,        /* Long press button to talk */\n    AI_AUDIO_ALERT_KEY_TALK,             /* Button talk */\n    AI_AUDIO_ALERT_WAKEUP_TALK,          /* Wakeup talk */\n    AI_AUDIO_ALERT_FREE_TALK,            /* Free talk */\n} AI_AUDIO_ALERT_TYPE_E;\n\n/**\n * @brief Plays an alert sound based on the specified alert type.\n *\n * @param type - The type of alert to play, defined by the APP_ALERT_TYPE enum.\n * @return OPERATE_RET - Returns OPRT_OK if the alert sound is successfully played, otherwise returns an error code.\n */\nOPERATE_RET ai_audio_player_play_alert(AI_AUDIO_ALERT_TYPE_E type);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"development-steps",children:"Development Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Call the module initialization interface, set the working mode, and register notification callbacks."}),"\n",(0,o.jsx)(n.li,{children:"Call the interface to open the audio module."}),"\n",(0,o.jsx)(n.li,{children:"If the working mode is manually triggered single-turn dialogue, the developer needs to call the manual start/stop voice input interface to control the timing of voice reporting. For other modes, the component will handle it internally."}),"\n",(0,o.jsx)(n.li,{children:"Developers can handle different events and states in the notification callback according to actual product requirements."}),"\n"]})]})}function c(e={}){let{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},97317:function(e,n,i){i.d(n,{A:()=>t});let t=i.p+"assets/images/asr_free_talk-3ca0f7ddefc524ec712924f0f8877d5b.svg"},79114:function(e,n,i){i.d(n,{A:()=>t});let t=i.p+"assets/images/asr_once_talk-4d0c25f2441fad5764c3cfd03ab4ccf3.svg"},58756:function(e,n,i){i.d(n,{A:()=>t});let t=i.p+"assets/images/manual_once_talk-51f0b84ad47078fc13d44ab6baadbdfc.svg"},22918:function(e,n,i){i.d(n,{A:()=>t});let t=i.p+"assets/images/vad_free_talk-2aba55a8eb820cbe1e04430bea75e4e6.svg"},84429:function(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(96540);let o={},l=t.createContext(o);function a(e){let n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(l.Provider,{value:n},e.children)}}}]);